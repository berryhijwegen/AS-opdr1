{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inleveropgave 2: Model-Free Prediction and Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze():\n",
    "    def __init__(self, rewards: np.array, start_pos: tuple, exit_positions: list, chance: float):\n",
    "        self.rewards = rewards\n",
    "        self.start_pos = start_pos\n",
    "        self.exit_positions = exit_positions\n",
    "        self.chance = chance\n",
    "\n",
    "        # bind moves to constant\n",
    "        self.actions = {\n",
    "            \"UP\": (-1, 0),\n",
    "            \"DOWN\": (1, 0),\n",
    "            \"LEFT\": (0, -1),\n",
    "            \"RIGHT\": (0, 1),\n",
    "        }\n",
    "\n",
    "    # Take probability into account while retrieving the next action\n",
    "    def get_actual_action(self, desired_action: str):\n",
    "        other_actions = list(set(self.actions.keys())- {desired_action})\n",
    "        chance_left = 1 - self.chance\n",
    "        chance_per_action = round(chance_left / len(other_actions), 1)\n",
    "        other_probabilities = [chance_per_action for _ in range(0, len(other_actions))]\n",
    "\n",
    "        return np.random.choice(\n",
    "            [desired_action, *other_actions],\n",
    "            replace=False,\n",
    "            p=[self.chance, *other_probabilities]\n",
    "        )\n",
    "    \n",
    "    # Get next position giving a certain action\n",
    "    def get_next_position(self, location: tuple, action: str):\n",
    "        x_dim, y_dim = self.rewards.shape\n",
    "        y, x = location\n",
    "        next_y, next_x = self.actions[action]\n",
    "        next_x += x\n",
    "        next_y += y\n",
    "\n",
    "        # Check if position is inside grid, else stay in same place\n",
    "        if (next_x >= 0 and next_x < x_dim and next_y >= 0 and next_y < y_dim):\n",
    "            return (next_y, next_x)\n",
    "\n",
    "        return location\n",
    "    \n",
    "    \n",
    "    def value_iteration(self, theta: float = 0.001):\n",
    "        n_actions = len(self.actions.keys())\n",
    "        V = np.zeros(self.rewards.shape)\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for y in range(self.rewards.shape[0]):\n",
    "                for x in range(self.rewards.shape[1]):  \n",
    "                    location = (y, x)\n",
    "                    if location in self.exit_positions:\n",
    "                        continue\n",
    "\n",
    "                    # Find best action\n",
    "                    A = self.lookahead_step(location, V)\n",
    "                    max_val = np.max(A)\n",
    "                    delta = max(delta, np.abs(max_val - V[location]))\n",
    "                    # Update value function\n",
    "                    V[location] = max_val\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        V = np.add(V, self.rewards)\n",
    "\n",
    "        # Generate policy by using optimal value function\n",
    "        policy = np.zeros([self.rewards.shape[0], self.rewards.shape[1], n_actions])\n",
    "        for y in range(self.rewards.shape[0]):\n",
    "            for x in range(self.rewards.shape[1]):\n",
    "                location = (y, x)\n",
    "\n",
    "                A = self.lookahead_step(location, V)\n",
    "                best_action = np.argmax(A)\n",
    "                policy[y, x, best_action] = 1.0\n",
    "\n",
    "        return policy, V\n",
    "\n",
    "    # Find most optimal step\n",
    "    def lookahead_step(self, location: tuple, V: np.array, discount_factor: float = 1.0):\n",
    "        n_actions = len(self.actions.keys())\n",
    "        A = np.zeros(n_actions)\n",
    "        a_idx = 0\n",
    "        for action in self.actions.keys():\n",
    "            next_state = self.get_next_position(location, action)\n",
    "            if next_state == location:\n",
    "                continue\n",
    "            A[a_idx] = self.chance * (self.rewards[next_state] + discount_factor * V[next_state])\n",
    "            a_idx += 1\n",
    "        return A\n",
    "    \n",
    "    def generate_random_policy(self, theta: float = 0.001):\n",
    "        n_actions = len(self.actions.keys())\n",
    "        # Generate policy by using optimal value function\n",
    "        policy = np.zeros([self.rewards.shape[0], self.rewards.shape[1], n_actions])\n",
    "        for y in range(self.rewards.shape[0]):\n",
    "            for x in range(self.rewards.shape[1]):\n",
    "                location = (y, x)\n",
    "\n",
    "                best_action = np.random.randint(0, n_actions)\n",
    "                policy[y, x, best_action] = 1.0\n",
    "\n",
    "        return policy\n",
    "    \n",
    "    def evaluate_policy(self, policy, n_episodes, discount_factor=1.0):\n",
    "        returns_sum = np.zeros(self.rewards.shape)\n",
    "        returns_count = np.zeros(self.rewards.shape)\n",
    "\n",
    "        V = np.zeros(self.rewards.shape)\n",
    "\n",
    "        for i_episode in range(1, n_episodes + 1):\n",
    "            # Generate an episode by running the run method this returns states, actions and rewards.\n",
    "            episode = self.run(policy)\n",
    "\n",
    "            states_in_episode = episode[0]\n",
    "            rewards_in_episode = episode[2]\n",
    "            for curr_state in states_in_episode:\n",
    "                # Get first occurance of state in this episode\n",
    "                first_occurence_idx = next(i for i,state in enumerate(states_in_episode) if curr_state == state)\n",
    "                # Sum up all rewards previous rewards\n",
    "                G = sum([reward*(discount_factor**i) for i,reward in enumerate(rewards_in_episode[first_occurence_idx:])])\n",
    "                # Calculate average return for this state over all sampled episodes\n",
    "                returns_sum[curr_state] += G\n",
    "                returns_count[curr_state] += 1.0\n",
    "                V[curr_state] = returns_sum[curr_state] / returns_count[curr_state]\n",
    "\n",
    "        return V\n",
    "    \n",
    "    def run(self, policy):\n",
    "        list_actions = list(self.actions.keys())\n",
    "        location = self.start_pos\n",
    "        states = [location]\n",
    "        actions = []\n",
    "        rewards = [0]\n",
    "        while True:\n",
    "            if location in self.exit_positions:\n",
    "                break\n",
    "            desired_action = np.argmax(policy[location])\n",
    "            actual_action = self.get_actual_action(list_actions[desired_action])\n",
    "            location = self.get_next_position(location, actual_action)\n",
    "            states.append(location)\n",
    "            actions.append(actual_action)\n",
    "            rewards.append(self.rewards[location])\n",
    "        return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pos = (3, 2)\n",
    "exit_positions = [(3, 0), (0, 3), (3, 1)]\n",
    "rewards = np.array([\n",
    "    [-1, -1, -1, 40],\n",
    "    [-1, -1, -10, -10],\n",
    "    [-1, -1, -1, -1],\n",
    "    [10, -2, -1, -1],\n",
    "])\n",
    "chance = .7\n",
    "maze = Maze(\n",
    "    rewards,\n",
    "    start_pos,\n",
    "    exit_positions,\n",
    "    chance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy, v = maze.value_iteration()\n",
    "# random_policy = maze.generate_random_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = maze.evaluate_policy(optimal_policy, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-31.27777778 -33.59649123 -12.72222222  40.        ]\n",
      " [-29.84516129 -29.83870968 -10.92        16.89622642]\n",
      " [ -5.11538462 -22.58823529  10.27884615  16.87962963]\n",
      " [ 10.          -2.           8.5203252   15.87096774]]\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 32-bit",
   "language": "python",
   "name": "python38032bit2cfc1b73ec5a4411aa7f17aefc596f40"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
